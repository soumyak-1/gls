{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyak-1/gls/blob/main/Book_To_Video_Course_Converter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mV20DPwS5Qg1",
        "outputId": "80decbfd-8aca-4a21-c02c-fd54d67fab0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package  pstotext\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/deanmalmgren/textract\n",
            "  Cloning https://github.com/deanmalmgren/textract to /tmp/pip-req-build-r7wtxma0\n",
            "  Running command git clone -q https://github.com/deanmalmgren/textract /tmp/pip-req-build-r7wtxma0\n",
            "Collecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from textract==1.6.5) (3.0.4)\n",
            "Collecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Collecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 60.1 MB/s \n",
            "\u001b[?25hCollecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 39.6 MB/s \n",
            "\u001b[?25hCollecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 72.6 MB/s \n",
            "\u001b[?25hCollecting xlrd~=1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 71.1 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20191110->textract==1.6.5) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4~=4.8.0->textract==1.6.5) (2.3.2.post1)\n",
            "Collecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "Collecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 36.5 MB/s \n",
            "\u001b[?25hCollecting tzlocal>=2.1\n",
            "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
            "Collecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 51.6 MB/s \n",
            "\u001b[?25hCollecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract==1.6.5) (4.9.1)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract==1.6.5) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 61.3 MB/s \n",
            "\u001b[?25hCollecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting tzdata\n",
            "  Downloading tzdata-2022.2-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[K     |████████████████████████████████| 336 kB 59.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: textract, docx2txt, compressed-rtf, olefile, python-pptx\n",
            "  Building wheel for textract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textract: filename=textract-1.6.5-py3-none-any.whl size=23140 sha256=28a7eeb1ce6d2a50842a777881adaa194959b5b6561a1757309efe0ecffac5da\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jy3q0krk/wheels/1b/10/32/71c9f6007c205a6c1c47735bf9c0e73575bdcf98716390b25d\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=a72e9c318d28694aff3bccd6a13319acee1095e4d82e476363eee463584f9d34\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6204 sha256=c0bcc9efe7217cc5a167f08242609fb9fdd3fc38909d3aaf7f92f27835d350f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/33/88/88ceee84d1b74b391c086bc594d3fcf80800decfbd6e1ff565\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=4fe8eb17d41381687d8d38a8c53b141e949d9b6446016ea4fa83bc3e17f308fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=5d0976e6834c1557bbf286021307df9cefd74fdab9218f532f4bef24155322be\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/ab/f4/52560d0d4bd4055e9261c6df6e51c7b56c2b23cca3dee811a3\n",
            "Successfully built textract docx2txt compressed-rtf olefile python-pptx\n",
            "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, XlsxWriter, tzlocal, pycryptodome, olefile, imapclient, ebcdic, compressed-rtf, xlrd, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.31.6 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.3 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.15.0 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 textract-1.6.5 tzdata-2022.2 tzlocal-4.2 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# !apt-get update         # To updates the package sources list to get the latest list of available packages in the repositories\n",
        "\n",
        "\n",
        "#Setup Textract for extracting text from different sources\n",
        "\n",
        "!apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils \\ pstotext tesseract-ocr \\\n",
        " flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig libasound2-dev libpulse-dev\n",
        "\n",
        "!pip install git+https://github.com/deanmalmgren/textract\n",
        "\n",
        "#Restart to bring-in effect Textract - You must restart the runtime in order to use newly installed versions.\n",
        "\n",
        "#  python-dev  -  It is the package that contains the header files for the Python C API, which is used by lxml because it includes Python C extensions for high performance.\n",
        "\n",
        "# libxslt1-dev   -  XSLT is an XML language for defining transformations of XML files from XML to some other arbitrary format, \n",
        "#                   such as XML, HTML, plain text, etc. using standard XSLT stylesheets. \n",
        "\n",
        "# antiword  -  Convert Word document to plaintext file\n",
        "\n",
        "# poppler-utils  -  Precompiled command-line utilities (based on Poppler) for manipulating PDF files and converting them to other formats\n",
        "\n",
        "# tesseract-ocr  -  OCR extracts text from images and documents without a text layer and outputs the document \n",
        "#                   into a new searchable text file, PDF, or most other popular formats.\n",
        "\n",
        "# flac  -  It allows you to encode and decode raw audio data directly to/from a file, or in real-time using callbacks.\n",
        "\n",
        "# ffmpeg  -  It is a command-line tool that converts audio or video formats. \n",
        "\n",
        "# lame  -  This library makes it simple to encode PCM data into MP3 without having to compile any binaries. \n",
        "\n",
        "# sox  -  It play and record audio files on most platforms.\n",
        "\n",
        "# swig  -  It provides capability to wrap c/c++ libraries with other languages such as Python, Ruby, Java etc.\n",
        "\n",
        "# libasound2-dev  - It required for developing software that makes use of libasound2, the ALSA library. ALSA is the Advanced Linux Sound Architecture.\n",
        "\n",
        "# libpulse-dev - It is a drop in replacement for the ESD sound server with much better latency, mixing/re-sampling quality and overall architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkOWvrv58h-j",
        "outputId": "8ae10bae-a5cf-4c80-ddaa-2a0a85465c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy) (1.21.6)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (2.9.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.64.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPdf2\n",
            "  Downloading PyPDF2-2.10.0-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from PyPdf2) (4.1.1)\n",
            "Installing collected packages: PyPdf2\n",
            "Successfully installed PyPdf2-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.2.4-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gTTS) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from gTTS) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gTTS) (1.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (1.24.3)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.2.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdf2jpg\n",
            "  Downloading pdf2jpg-1.1-py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 31.5 MB/s \n",
            "\u001b[?25hCollecting img2pdf\n",
            "  Downloading img2pdf-0.4.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from img2pdf->pdf2jpg) (7.1.2)\n",
            "Collecting pikepdf\n",
            "  Downloading pikepdf-5.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 51.0 MB/s \n",
            "\u001b[?25hCollecting deprecation\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: lxml>=4.0 in /usr/local/lib/python3.7/dist-packages (from pikepdf->img2pdf->pdf2jpg) (4.9.1)\n",
            "Requirement already satisfied: importlib-metadata>=4 in /usr/local/lib/python3.7/dist-packages (from pikepdf->img2pdf->pdf2jpg) (4.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pikepdf->img2pdf->pdf2jpg) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.7/dist-packages (from pikepdf->img2pdf->pdf2jpg) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4->pikepdf->img2pdf->pdf2jpg) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pikepdf->img2pdf->pdf2jpg) (3.0.9)\n",
            "Building wheels for collected packages: img2pdf\n",
            "  Building wheel for img2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for img2pdf: filename=img2pdf-0.4.4-py3-none-any.whl size=44881 sha256=8883350053570e6320d64b0485da8764af931141038924d81ee4f2fd07b3335d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/a7/53/9983aef9861f91881984e08f376e3119bdfeeecf55bd36e585\n",
            "Successfully built img2pdf\n",
            "Installing collected packages: deprecation, pikepdf, img2pdf, pdf2jpg\n",
            "Successfully installed deprecation-2.1.0 img2pdf-0.4.4 pdf2jpg-1.1 pikepdf-5.4.2\n"
          ]
        }
      ],
      "source": [
        "# video generator from images\n",
        "\n",
        "!pip install Pillow                   # It contains all the basic image processing functionality. \n",
        "!pip install opencv-python            # It can process images and videos to identify objects, faces, or even the handwriting of a human.\n",
        "!pip install moviepy                  # It is used for video editing: cutting, concatenations, title insertions, etc.\n",
        "!pip install PyPdf2                   # It is used for performing major tasks on PDF files such as extracting, merging, splitting, etc.\n",
        "!pip install gTTS                     # It is used for speech translation\n",
        "!pip install pdf2jpg                  # Wrapper to convert PDF files into jpg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlKG5o44cBBK",
        "outputId": "bbe4422d-4ca0-4c55-ad82-9acaeceb93ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 90.4 MB of archives.\n",
            "After this operation, 306 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 103.0.5060.134-0ubuntu0.18.04.1 [1,160 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 103.0.5060.134-0ubuntu0.18.04.1 [79.0 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 103.0.5060.134-0ubuntu0.18.04.1 [5,043 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 103.0.5060.134-0ubuntu0.18.04.1 [5,202 kB]\n",
            "Fetched 90.4 MB in 3s (28.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155680 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_103.0.5060.134-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.4.0-py3-none-any.whl (985 kB)\n",
            "\u001b[K     |████████████████████████████████| 985 kB 27.6 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 52.6 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\n",
            "google-api-core 1.31.6 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.4.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.11 wsproto-1.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!apt install chromium-chromedriver                       # It provides capabilities for navigating to web pages, user input, JavaScript execution, and more.\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium                                    # To automate the testing across various web browsers.\n",
        "\n",
        "import nltk                                              # It provides us various text processing libraries with a lot of test datasets.\n",
        "nltk.download(\"popular\")                                 # It will download a list of \"popular\" resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RV5fYiqfZDb",
        "outputId": "cafd8dcf-c230-4855-c6af-b8fbd0a2d300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize      #  To split a document or paragraph into sentences.\n",
        "from nltk.tokenize import TweetTokenizer     #  convert the stream of words into small tokens so that we can analyse the audio stream\n",
        "import string\n",
        "from nltk.corpus import stopwords            #  These are the English words which does not add much meaning to a sentence.For eg, the words like the, he, have etc.\n",
        "import requests\n",
        "import json\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "from pdf2jpg import pdf2jpg\n",
        "import os, sys, stat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlEbuL8LiRFX"
      },
      "outputs": [],
      "source": [
        "# Generate an audio content\n",
        "# Record the voice\n",
        "\n",
        "from gtts import gTTS\n",
        "from time import sleep\n",
        "import os\n",
        "from IPython.display import Audio\n",
        "from nltk.tag.perceptron import PerceptronTagger\n",
        "\n",
        "# Video Libraries\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from os import walk\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQpgdt2HHb36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0a7fda-704a-4dff-e40e-b275e40eb5ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio==2.4.1\n",
            "  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.1) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.1) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303885 sha256=67c81dbc3b66a76030445466359889b049c42b7bd3015f7fbdf0ac0dc6510c10\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/20/07/7bb9c8c44e6ec2efa60fd0e6280094f53f65f41767ef69a5ee\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "Successfully installed imageio-2.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install imageio==2.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNfAqdzHk9kS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9691d7da-11e3-4deb-b9f8-7350dc720aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3612672/45929032 bytes (7.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7077888/45929032 bytes (15.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10166272/45929032 bytes (22.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13639680/45929032 bytes (29.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16596992/45929032 bytes (36.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20258816/45929032 bytes (44.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23707648/45929032 bytes (51.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27303936/45929032 bytes (59.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30588928/45929032 bytes (66.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34136064/45929032 bytes (74.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37584896/45929032 bytes (81.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40861696/45929032 bytes (89.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44310528/45929032 bytes (96.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        }
      ],
      "source": [
        "# Create production video\n",
        "\n",
        "import moviepy.editor as mpe\n",
        "\n",
        "\n",
        "\n",
        "def makeAudio(pagetext, page_no): \n",
        "  text = remove_punctuations_audio(pagetext)  \n",
        "  print(text) \n",
        "  tts = gTTS(text=text, lang=\"en\", tld=\"co.in\", slow=False)\n",
        "  filename = \"page_\"+ str(page_no) + '.mp3'\n",
        "  os.chdir(path_audio)\n",
        "  tts.save(filename)\n",
        "\n",
        "def playLesson(page_file):  \n",
        "  os.chdir(path)\n",
        "  Audio(page_file, autoplay=True)\n",
        "  \n",
        "def downloadImage(image_url, image_no): \n",
        "\n",
        "  # Open the url image, set stream to True, this will return the stream content.\n",
        "  resp = requests.get(image_url, stream=True)\n",
        "\n",
        "  # Open a local file with wb ( write binary ) permission.\n",
        "  local_file = open(str(image_no)+'.jpg', 'wb')\n",
        "\n",
        "  # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
        "  resp.raw.decode_content = True\n",
        "\n",
        "  # Copy the response stream raw data to local image file.\n",
        "  shutil.copyfileobj(resp.raw, local_file)\n",
        "  \n",
        "  # Remove the image url response object.\n",
        "  del resp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBAqtd7YlULF"
      },
      "outputs": [],
      "source": [
        "def remove_punctuations(text):  \n",
        "  words = nltk.word_tokenize(text)\n",
        "  punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
        "  return \" \".join(punt_removed)\n",
        "\n",
        "def remove_punctuations_audio(text):\n",
        "  words = nltk.word_tokenize(text)  \n",
        "  punct = \"!\\\"#$&();<>[\\\\]^{|}~_\"\n",
        "  punt_removed = [w for w in words if w.lower() not in punct]\n",
        "  return \" \".join(punt_removed)\n",
        "\n",
        "def remove_stopwords(text, lang='english'):\n",
        "  words = nltk.word_tokenize(text)\n",
        "  lang_stopwords = stopwords.words(lang)\n",
        "  stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
        "  return \" \".join(stopwords_removed)\n",
        "\n",
        "def remove_whitespace(text):\n",
        "  return \" \".join(text.split())\n",
        "\n",
        "def freq(str):\n",
        "\n",
        "    # break the string into list of words \n",
        "    str_list = str.split()\n",
        "\n",
        "    # gives set of unique words \n",
        "    unique_words = set(str_list)\n",
        "\n",
        "    for words in unique_words : \n",
        "        print('Frequency of ', words , 'is :', str_list.count(words)) \n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdt6Zs3QluH5"
      },
      "outputs": [],
      "source": [
        "# Download images from internet depending on the content\n",
        "# Connect to google.com and search the keyword\n",
        "google_url = \"https://www.googleapis.com/customsearch/v1/?key=AIzaSyChpbRKlIqrtLhmT3I9wPTCeb7cH8Hd5y0&cx=018372273573726385166:uetxc7txscc\"\n",
        "\n",
        "def searchRequest(searchQuery, start=\"1\"):\n",
        "  search_request = google_url + \"&q=\" + searchQuery + \"&searchType=image&fileType=jpg&imgSize=large&alt=json\"\n",
        "  return search_request\n",
        "\n",
        "def searchImage(pagetext):   \n",
        "  searchQuery = \"\"\n",
        "  \n",
        "  text = remove_punctuations(pagetext)\n",
        "  text = remove_stopwords(text)\n",
        "  # text = remove_whitespace \n",
        "  print(text)\n",
        "  tt = TweetTokenizer()  \n",
        "  tokens = tt.tokenize(text)\n",
        "  \n",
        "  searchterms = []\n",
        "  \n",
        "  for token in tokens:\n",
        "    token = re.sub('[^A-Za-z0-9]+', '', token)\n",
        "    searchterms.append(token)\n",
        "    \n",
        "  searchword = []\n",
        "  token_result = Counter(searchterms)  \n",
        "  for key, value in sorted(token_result.items(), key=lambda kv: kv[1], reverse=True):\n",
        "    searchword.append(key)\n",
        "  \n",
        "  for i in range(0, 6):   \n",
        "    try:\n",
        "      searchQuery = searchQuery + \" intext:\" + searchword[i]\n",
        "    except:\n",
        "      pass   \n",
        "\n",
        "  print(\"Searching: \", searchQuery)  \n",
        "  # Send the request to server\n",
        "  response = requests.get(searchRequest(searchQuery))\n",
        "  json_response = json.loads(response.text)\n",
        "  print(json_response)\n",
        "\n",
        "  total_results = int(json_response[\"searchInformation\"][\"totalResults\"])\n",
        "  try:\n",
        "    next_index = int(json_response[\"queries\"][\"nextPage\"][0][\"startIndex\"])\n",
        "  except:\n",
        "    next_index = 11\n",
        "  total_pages = round(total_results / 10)\n",
        "\n",
        "  course_image_data = {}\n",
        "  l_title = []\n",
        "  l_url = []\n",
        "  try:\n",
        "    while next_index < total_pages: \n",
        "      for item in json_response[\"items\"]:  \n",
        "        l_url.append(item[\"link\"])\n",
        "        l_title.append(item[\"title\"])  \n",
        "      response = requests.get(searchRequest(start=str(next_index)))\n",
        "      json_response = json.loads(response.text)\n",
        "      print(json_response)\n",
        "      next_index = json_response[\"queries\"][\"nextPage\"][0][\"startIndex\"]\n",
        "      print(next_index)\n",
        "  except:\n",
        "    print(\"Search Complete Exception\")\n",
        "  course_image_data[\"Title\"] = l_title\n",
        "  course_image_data[\"Url\"] = l_url\n",
        "    # college_data\n",
        "  imagedata = pd.DataFrame(data=course_image_data)\n",
        "  print(\"Data collected for %d images\" % imagedata.shape[0])\n",
        "  return course_image_data[\"Url\"]\n",
        "\n",
        "def createImage(page_no, inputpath, outputpath):\n",
        "  imageList = []\n",
        "  result = pdf2jpg.convert_pdf2jpg(inputpath, outputpath,  pages=str(page_no))\n",
        "  print(\"CreateImage result\", result)\n",
        "  return imageList.append(str(page_no)+\"_\"+doc_name+\".jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4qrDWQWmSJd"
      },
      "outputs": [],
      "source": [
        "# Video Generation from Images\n",
        "\n",
        "# importing libraries \n",
        "\n",
        "# Video Generating function \n",
        "def generate_video(page_no, files): \n",
        "  image_folder = path_resized+\"/\"+str(page_no) # make sure to use your folder \n",
        "  video_name = path_video + \"/\"+str(page_no)+'.mp4'\n",
        "  # os.chdir(path_video) \n",
        "\n",
        "  images = [img for img in files \n",
        "            if img.endswith(\".jpg\") or\n",
        "               img.endswith(\".jpeg\") or\n",
        "               img.endswith(\"png\")] \n",
        "\n",
        "  # Array images should only consider \n",
        "  # the image files ignoring others if any \n",
        "  print(images)  \n",
        "    \n",
        "  frame = cv2.imread(os.path.join(image_folder, images[0])) \n",
        "\n",
        "  # setting the frame width, height width \n",
        "  # the width, height of first image \n",
        "  height, width, layers = frame.shape   \n",
        "\n",
        "  #video = cv2.VideoWriter(video_name, 0, 1, (width, height)) \n",
        "  video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'DIVX'), 60, (width, height)) \n",
        "\n",
        "    # Appending the images to the video one by one \n",
        "  for image in images:  \n",
        "      print(\"Adding \", image, \" to video\")\n",
        "      img = cv2.imread(os.path.join(image_folder, image))\n",
        "      cv2.putText(img, \"hackveda.in\", (width - 150, round(height / 8)), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), lineType=cv2.LINE_AA)\n",
        "      video.write(img)         \n",
        "\n",
        "  # Deallocating memories taken for window creation \n",
        "  cv2.destroyAllWindows()  \n",
        "  video.release()  # releasing the video generated \n",
        "  print(\"Video ready\", video_name)\n",
        "\n",
        "    \n",
        "def makeCourseVideo(imagePath, page_no):\n",
        "  path = imagePath\n",
        "  mean_height = 0\n",
        "  mean_width = 0\n",
        "  files= []\n",
        "\n",
        "  \"\"\" Change 5 - Comment / Uncomment \"\"\"\n",
        "  #for (dirpath, dirnames, filenames) in walk(path_images+\"/\"+str(page_no)):\n",
        "  for (dirpath, dirnames, filenames) in walk(path_images+\"/\"+doc_name+\"_dir\"):\n",
        "    files.extend(filenames)\n",
        "    break\n",
        "  print(files)\n",
        "\n",
        "  num_of_images = len(files)\n",
        "\n",
        "\n",
        "  for file in files: \n",
        "    try:\n",
        "      print(\"Test\", file)\n",
        "      im = Image.open(os.path.join(path, file)) \n",
        "      width, height = im.size \n",
        "      mean_width += width \n",
        "      mean_height += height \n",
        "    except:\n",
        "      print(\"Image Exception\")\n",
        "  # im.show()   # uncomment this for displaying the image \n",
        "\n",
        "  # Finding the mean height and width of all images. \n",
        "  # This is required because the video frame needs \n",
        "  # to be set with same width and height. Otherwise \n",
        "  # images not equal to that width height will not get  \n",
        "  # embedded into the video \n",
        "  mean_width = int(mean_width / num_of_images) \n",
        "  mean_height = int(mean_height / num_of_images) \n",
        "\n",
        "  # print(mean_height) \n",
        "  # print(mean_width) \n",
        "\n",
        "  # Resizing of the images to give \n",
        "  # them same width and height  \n",
        "  for file in files: \n",
        "    try:\n",
        "      if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\"png\"): \n",
        "      \n",
        "        # opening image using PIL Image \n",
        "        im = Image.open(os.path.join(path, file))\n",
        "\n",
        "        # im.size includes the height and width of image \n",
        "        width, height = im.size    \n",
        "        print(width, height) \n",
        "\n",
        "        # resizing  \n",
        "        imResize = im.resize((mean_width, mean_height), Image.ANTIALIAS)\n",
        "\n",
        "        imResize.save(path_resized + \"/\" + str(page_no) + \"/\" + file, 'JPEG', quality = 95) # setting quality \n",
        "        \n",
        "        # printing each resized image name \n",
        "        print(im.filename.split('\\\\')[-1], \" is resized\")  \n",
        "         \n",
        "        \n",
        "    except:\n",
        "      print(\"Image Exception\")\n",
        "\n",
        "  # Calling the generate_video function\n",
        "  generate_video(page_no, files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9joILxqsmxef"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-enwVmuNnEyG"
      },
      "outputs": [],
      "source": [
        "def makeProductionVideo(path_final, page_no):\n",
        "  \n",
        "  my_clip = mpe.VideoFileClip(path_video+\"/\"+str(page_no)+'.mp4')\n",
        "  audio_background = mpe.AudioFileClip(path_audio+\"/page_\"+str(page_no)+'.mp3')\n",
        "  final_audio = mpe.CompositeAudioClip([audio_background])\n",
        "  final_clip = my_clip.set_audio(final_audio)\n",
        "\n",
        "  # audio_codec=\"aac\" could make final_clip played in iPhone/Android/PC\n",
        "  final_clip.write_videofile(path_final+\"/\" + \"Lesson \" + str(page_no + 1)+ \"-\" + course + \".mp4\", audio_codec=\"aac\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHiTVDpBnpSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6efccf1-2bfa-48c7-ad8c-331ebe2cd9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages 2\n",
            "Enter course transcript name: test\n",
            "Enter start page no:0\n",
            "Enter end page no:1\n",
            "Reading page number 0\n",
            "Reading page number 1\n"
          ]
        }
      ],
      "source": [
        "# Create a transcript\n",
        "\n",
        "import PyPDF2 \n",
        "import textract\n",
        "\n",
        "doc_name = \"About_Us_-_Hackveda_Limited.pdf\"\n",
        "\n",
        "# creating a pdf file object \n",
        "#filename = input(\"Enter filename: \")\n",
        "pdfFileObj = open(\"/content/\"+doc_name, 'rb') \n",
        "  \n",
        "# creating a pdf reader object \n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
        "  \n",
        "# printing number of pages in pdf file \n",
        "number_of_pages = pdfReader.numPages\n",
        "print(\"Number of pages\", number_of_pages)\n",
        "\n",
        "# Start reading from page 0\n",
        "page_no = 0\n",
        "\n",
        "course = input(\"Enter course transcript name: \")\n",
        "path = \"/content/\"+course\n",
        "path_transcript = path + \"/transcript\"\n",
        "\n",
        "os.mkdir(path)\n",
        "os.mkdir(path_transcript)\n",
        "\n",
        "startpage = int(input(\"Enter start page no:\"))\n",
        "endpage = int(input(\"Enter end page no:\"))\n",
        "\n",
        "while(page_no < number_of_pages):\n",
        "  if page_no >= startpage and page_no <= endpage:\n",
        "    pagetext = \"\"\n",
        "\n",
        "    # creating a page object \n",
        "    pageObj = pdfReader.getPage(page_no)\n",
        "\n",
        "    print(\"Reading page number\", page_no)\n",
        "    pagetext = pagetext + \" \" + pageObj.extractText() \n",
        "\n",
        "    if pagetext.strip() == \"\":\n",
        "      pdfwriter = PyPDF2.PdfFileWriter()\n",
        "      pdfwriter.addPage(pdfReader.getPage(page_no))\n",
        "      with open('/content/tmp.pdf', 'wb') as f:\n",
        "        pdfwriter.write(f)\n",
        "        f.close()\n",
        "      pagetext = textract.process(\"/content/tmp.pdf\").decode(\"UTF-8\")\n",
        "\n",
        "    pagetext = remove_punctuations_audio(pagetext)\n",
        "    \n",
        "    transfile = open(path_transcript + \"/\" + str(page_no) + \".txt\", 'w')  \n",
        "    print(pagetext, file=transfile, flush=True)  \n",
        "  page_no = page_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYQuedxBUTbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249b1c48-ae53-47d1-85c6-62f96d242a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Enter course name: abc\n",
            "Hi, I am Soumyak from . In this video i have brought to you, Hackveda Limited Devanshu Shukla , Founder Director 011-27297608 Hackveda Delhi - H-3/60 , III Floor , Sector-18 , Rohini , Delhi - 110089 Hackveda Noida - C-25 , Amigo Building , First Floor , Sector-8 , Noida , UP - 201301 Hackveda Canada - 615-50 , Dunsheath Way , Markham , Ontario , L6B1N3 3 November 2020 About Hackveda Limited Workforce Development , Software Development , Research and Development , AWS Consultancy , IoT Development , Digital Ads Network , Afﬁliate Marketing , Recruitment and Talent Acquisition Hackveda Limited was founded on 16/11/2018 as an International Business Corporation under Canada Business Corporations Act . Hackveda Limited was started by Mr Devanshu Shukla , Sole Proprietor Active Director Hackveda - Software Training Company , Registered under Goods and Services Act , Delhi . Hackveda Limited develops provide products and services to various verticals including but not limited to Workforce Development , Software Development , Research and Development , AWS Consultancy , IoT Development , Digital Ads Network , Afﬁliate Marketing , Recruitment and Talent Acquisition . Workforce Development and Talent Acquisition Services are the core of our services . We train freshman , provide industrial application development experiences , certify them with international industrial and academic certiﬁcations . Our services are available as FREE for Self Learning Candidates as well as we do offer Instructor Led Trainings as high quality and competitive prices . We have served 250+ colleges , government agencies in India for technology trainings . Software Development services includes high quality web and mobile application development . We deal with Govt Agencies including DRDO , ADRDE , Ministry of Home Affairs , Ministry of Education etc . We have been working with private small and medium HACKVEDA LIMITED - ABOUT US 1\n",
            "\n",
            "Hi , I am Soumyak from . In this video i have brought to you , Hackveda Limited Devanshu Shukla , Founder Director 011-27297608 Hackveda Delhi - H-3/60 , III Floor , Sector-18 , Rohini , Delhi - 110089 Hackveda Noida - C-25 , Amigo Building , First Floor , Sector-8 , Noida , UP - 201301 Hackveda Canada - 615-50 , Dunsheath Way , Markham , Ontario , L6B1N3 3 November 2020 About Hackveda Limited Workforce Development , Software Development , Research and Development , AWS Consultancy , IoT Development , Digital Ads Network , Afﬁliate Marketing , Recruitment and Talent Acquisition Hackveda Limited was founded on 16/11/2018 as an International Business Corporation under Canada Business Corporations Act . Hackveda Limited was started by Mr Devanshu Shukla , Sole Proprietor Active Director Hackveda - Software Training Company , Registered under Goods and Services Act , Delhi . Hackveda Limited develops provide products and services to various verticals including but not limited to Workforce Development , Software Development , Research and Development , AWS Consultancy , IoT Development , Digital Ads Network , Afﬁliate Marketing , Recruitment and Talent Acquisition . Workforce Development and Talent Acquisition Services are the core of our services . We train freshman , provide industrial application development experiences , certify them with international industrial and academic certiﬁcations . Our services are available as FREE for Self Learning Candidates as well as we do offer Instructor Led Trainings as high quality and competitive prices . We have served 250+ colleges , government agencies in India for technology trainings . Software Development services includes high quality web and mobile application development . We deal with Govt Agencies including DRDO , ADRDE , Ministry of Home Affairs , Ministry of Education etc . We have been working with private small and medium HACKVEDA LIMITED - ABOUT US 1\n",
            "CreateImage result [{'cmd': ['java', '-jar', '/usr/local/lib/python3.7/dist-packages/pdf2jpg/pdf2jpg.jar', '-i', '/content/About_Us_-_Hackveda_Limited.pdf', '-o', '/content/abc/images', '-d', '300', '-p', '0'], 'input_path': '/content/About_Us_-_Hackveda_Limited.pdf', 'output_pdfpath': '/content/abc/images/About_Us_-_Hackveda_Limited.pdf_dir', 'output_jpgfiles': ['/content/abc/images/About_Us_-_Hackveda_Limited.pdf_dir/0_About_Us_-_Hackveda_Limited.pdf.jpg']}]\n",
            "None\n",
            "['0_About_Us_-_Hackveda_Limited.pdf.jpg']\n",
            "Test 0_About_Us_-_Hackveda_Limited.pdf.jpg\n",
            "2479 3508\n",
            "/content/abc/images/About_Us_-_Hackveda_Limited.pdf_dir/0_About_Us_-_Hackveda_Limited.pdf.jpg  is resized\n",
            "['0_About_Us_-_Hackveda_Limited.pdf.jpg']\n",
            "Adding  0_About_Us_-_Hackveda_Limited.pdf.jpg  to video\n",
            "Video ready /content/abc/video/0.mp4\n",
            "[MoviePy] >>>> Building video /content/abc_final/Lesson 1-abc.mp4\n",
            "[MoviePy] Writing audio in Lesson 1-abcTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3875/3875 [00:09<00:00, 422.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MoviePy] Done.\n",
            "[MoviePy] Writing video /content/abc_final/Lesson 1-abc.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]WARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/moviepy/video/io/ffmpeg_reader.py:130: UserWarning: Warning: in file /content/abc/video/0.mp4, 26078472 bytes wanted but 0 bytes read,at frame 1/2, at time 0.02/0.02 sec. Using the last valid frame instead.\n",
            "  UserWarning)\n",
            "\n",
            "100%|██████████| 2/2 [00:00<00:00, 21.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MoviePy] Done.\n",
            "[MoviePy] >>>> Video ready: /content/abc_final/Lesson 1-abc.mp4 \n",
            "\n",
            "scale companies also and our products and services ranges upto 25L . We provide software solutions to majorly all kinds of Physical and Online Business . Research and Development services includes PHD Research Support and Writing services . We offer research application development , research paper writing , thesis writing . Our PHD Research Support services Ranges upto 2.5L AWS Consultancy is a promising vertical where we deal with startups , small and medium scale companies to support them in maintaining their applications on AWS Cloud easily . We are working on 100+ AWS Services and provide consultancy to effective use these services in various business models across the Globe . IoT Development services includes development of hardware including paratrooper locator system , card based access management solutions , intelligent body composition machines , healthcare and ﬁtness sensor based apps . Digital Ads and Afﬁliate Marketing section deals with existing business solution providers . We support in building a global digital marketing strategy for them to generate sales . These strategies include plan , execution of ads to generate organic and paid trafﬁc along with improved sales . We market existing software and information technology products and applications as an afﬁliate marketer . Recruitment and Talent acquisition ﬁnds out the job requirements and develop best suitable candidates for the hiring partner . We ensure that each technical , educational , professional and communication requirements of the hiring partner is met with our right talent development and identiﬁcation services . HACKVEDA LIMITED - ABOUT US 2\n",
            "Thanks for watching. See you in the next video.\n",
            "scale companies also and our products and services ranges upto 25L . We provide software solutions to majorly all kinds of Physical and Online Business . Research and Development services includes PHD Research Support and Writing services . We offer research application development , research paper writing , thesis writing . Our PHD Research Support services Ranges upto 2.5L AWS Consultancy is a promising vertical where we deal with startups , small and medium scale companies to support them in maintaining their applications on AWS Cloud easily . We are working on 100+ AWS Services and provide consultancy to effective use these services in various business models across the Globe . IoT Development services includes development of hardware including paratrooper locator system , card based access management solutions , intelligent body composition machines , healthcare and ﬁtness sensor based apps . Digital Ads and Afﬁliate Marketing section deals with existing business solution providers . We support in building a global digital marketing strategy for them to generate sales . These strategies include plan , execution of ads to generate organic and paid trafﬁc along with improved sales . We market existing software and information technology products and applications as an afﬁliate marketer . Recruitment and Talent acquisition ﬁnds out the job requirements and develop best suitable candidates for the hiring partner . We ensure that each technical , educational , professional and communication requirements of the hiring partner is met with our right talent development and identiﬁcation services . HACKVEDA LIMITED - ABOUT US 2 Thanks for watching . See you in the next video .\n",
            "CreateImage result [{'cmd': ['java', '-jar', '/usr/local/lib/python3.7/dist-packages/pdf2jpg/pdf2jpg.jar', '-i', '/content/About_Us_-_Hackveda_Limited.pdf', '-o', '/content/abc/images', '-d', '300', '-p', '1'], 'input_path': '/content/About_Us_-_Hackveda_Limited.pdf', 'output_pdfpath': '/content/abc/images/About_Us_-_Hackveda_Limited.pdf_dir', 'output_jpgfiles': ['/content/abc/images/About_Us_-_Hackveda_Limited.pdf_dir/1_About_Us_-_Hackveda_Limited.pdf.jpg']}]\n",
            "None\n",
            "['1_About_Us_-_Hackveda_Limited.pdf.jpg']\n",
            "Test 1_About_Us_-_Hackveda_Limited.pdf.jpg\n",
            "2479 3508\n",
            "/content/abc/images/About_Us_-_Hackveda_Limited.pdf_dir/1_About_Us_-_Hackveda_Limited.pdf.jpg  is resized\n",
            "['1_About_Us_-_Hackveda_Limited.pdf.jpg']\n",
            "Adding  1_About_Us_-_Hackveda_Limited.pdf.jpg  to video\n",
            "Video ready /content/abc/video/1.mp4\n",
            "[MoviePy] >>>> Building video /content/abc_final/Lesson 2-abc.mp4\n",
            "[MoviePy] Writing audio in Lesson 2-abcTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2994/2994 [00:06<00:00, 444.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MoviePy] Done.\n",
            "[MoviePy] Writing video /content/abc_final/Lesson 2-abc.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]WARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/moviepy/video/io/ffmpeg_reader.py:130: UserWarning: Warning: in file /content/abc/video/1.mp4, 26078472 bytes wanted but 0 bytes read,at frame 1/2, at time 0.02/0.02 sec. Using the last valid frame instead.\n",
            "  UserWarning)\n",
            "\n",
            "100%|██████████| 2/2 [00:00<00:00, 23.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MoviePy] Done.\n",
            "[MoviePy] >>>> Video ready: /content/abc_final/Lesson 2-abc.mp4 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get content from files\n",
        "# importing required modules \n",
        "\n",
        "\n",
        "import PyPDF2 \n",
        "import textract\n",
        "\n",
        "doc_name = \"About_Us_-_Hackveda_Limited.pdf\"\n",
        "#path_transcript = \"/content/Font Matching Using Model Fine Tuning/transcript\"\n",
        "\n",
        "# creating a pdf file object \n",
        "\n",
        "#filename = input(\"Enter filename: \")\n",
        "pdfFileObj = open(\"/content/\"+doc_name, 'rb') \n",
        "  \n",
        "# creating a pdf reader object \n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
        "  \n",
        "# printing number of pages in pdf file \n",
        "number_of_pages = pdfReader.numPages\n",
        "print(number_of_pages)\n",
        "pagetext_list = []\n",
        "\n",
        "# Start reading from page 0\n",
        "page_no = 0\n",
        "start_page = page_no\n",
        "last_page = number_of_pages - 1\n",
        "\n",
        "course = input(\"Enter course name: \")\n",
        "path = \"/content/\"+course\n",
        "path_images = path +\"/images\"\n",
        "path_resized = path +\"/resized\"\n",
        "path_audio = path +\"/audio\"\n",
        "path_video = path +\"/video\"\n",
        "path_final = path + \"_final\"\n",
        "\n",
        "# Create a course director\n",
        "\n",
        "os.mkdir(path)\n",
        "os.mkdir(path_images)\n",
        "os.mkdir(path_resized)\n",
        "os.mkdir(path_audio)\n",
        "os.mkdir(path_video)\n",
        "os.mkdir(path_final)\n",
        "\n",
        "os.chmod(path_images, stat.S_IRWXO)\n",
        "os.chmod(path_video, 777)\n",
        "os.chmod(path_final, 777)\n",
        "\n",
        "# Read all pages\n",
        "while(page_no < number_of_pages):\n",
        "\n",
        "  if page_no >= startpage and page_no <= endpage:\n",
        "    pagetext = \"\"\n",
        "\n",
        "    # creating a page object \n",
        "    pageObj = pdfReader.getPage(page_no)\n",
        "\n",
        "    \"\"\"# extracting text from page \n",
        "    if page_no != 0:\n",
        "      pagetext = pagetext + \" \" + pageObj.extractText()\n",
        "    if page_no == last_page:\n",
        "      pagetext = pagetext + \". Thanks for listening. Visit us on www.hackveda.in and Talk to Mentor\"\n",
        "    else:\n",
        "      pagetext = pagetext + \"Hi, I am an AI Teacher from Hackveda. Today I have brought for you \" + pageObj.extractText()\n",
        "  \"\"\" \n",
        "    \n",
        "    # Extract text from transcript\n",
        "    t_file = open(path_transcript + \"/\" + str(page_no) +\".txt\", \"r\")\n",
        "    pagetext = t_file.read()\n",
        "\n",
        "    # Add a personnel message\n",
        "    if page_no == start_page:\n",
        "      pagetext = \"Hi, I am Soumyak from . In this video i have brought to you, \" + pagetext\n",
        "    \n",
        "    if page_no == last_page:\n",
        "      pagetext = pagetext + \"Thanks for watching. See you in the next video.\"\n",
        "\n",
        "      #pagetext = pagetext + \"If you learned something, please subscribe share and like the videos. \"\n",
        "      #pagetext = pagetext + \"Checkout our playlists, you will get 100% hands on sessions with our mentors. Thank you\"\n",
        "\n",
        "    else:\n",
        "\n",
        "      #pagetext = pagetext + \"See you in the next video. Thank you.\"\n",
        "      pass\n",
        "  \n",
        "    \n",
        "    print(pagetext)\n",
        "    \n",
        "\n",
        "    # Create a page audio\n",
        "    try:\n",
        "      makeAudio(pagetext, page_no) \n",
        "    except Exception as e:\n",
        "      makeAudio(pagetext, page_no) \n",
        "      print(\"Exception audio:\", e)\n",
        "    imageCounter = 1\n",
        "    \n",
        "    # Search Images\n",
        "    \"\"\" Change 1 \"\"\"\n",
        "\n",
        "    #imageList = searchImage(pagetext)\n",
        "    imageList = createImage(page_no, \"/content/\"+doc_name, path_images)\n",
        "    print(imageList)\n",
        "      \n",
        "    # Make Images Directory for page\n",
        "    \"\"\" Change 2 - Comment(Image of PDF) / Uncomment (Google Image Search) \"\"\"\n",
        "\n",
        "    #os.mkdir(path_images+\"/\"+str(page_no))\n",
        "    os.mkdir(path_resized+\"/\"+str(page_no))\n",
        "    \n",
        "    \"\"\" Change 3 - Comment (Image for PDF) / Uncomment (Google Image Search) -> for loop block\"\"\"\n",
        "    \"\"\"for imageLink in imageList:\n",
        "      # Download Images\n",
        "      downloadImage(imageLink, path_images+\"/\" + str(page_no) + \"/\" + str(imageCounter))\n",
        "      imageCounter = imageCounter + 1\"\"\"\n",
        "    \n",
        "    # Make page video\n",
        "    \"\"\" Change 4 - Comment / Uncomment\"\"\"\n",
        "    \n",
        "    #makeCourseVideo(path_images + \"/\" + str(page_no), page_no)\n",
        "    makeCourseVideo(path_images + \"/\" + doc_name+\"_dir\", page_no)\n",
        "    \n",
        "    # Make production video\n",
        "    makeProductionVideo(path_final, page_no)\n",
        "    \n",
        "  # increment page_no\n",
        "  page_no = page_no + 1\n",
        "      \n",
        "# closing the pdf file object \n",
        "pdfFileObj.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nioRx9lVjvM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "64bab144-d19b-4eaf-e2ac-1cd4cc5d2722"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!chmod 777 -R \"/content/Scale for Success Part 2\"\\n!chmod 777 -R \"/content/Scale for Success Part 2_final\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "!chmod 777 -R \"/content/\"{course}\n",
        "!chmod 777 -R \"/content/\"{course}\"_final\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "!chmod 777 -R \"/content/Scale for Success Part 2\"\n",
        "!chmod 777 -R \"/content/Scale for Success Part 2_final\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCB3P7xjV-nO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3bb7ad6b-87de-40da-9661-520408e2dcd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/abc_final/ (stored 0%)\n",
            "  adding: content/abc_final/Lesson 1-abc.mp4 (deflated 7%)\n",
            "  adding: content/abc_final/Lesson 2-abc.mp4 (deflated 7%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!zip -r /content/CourseFile.zip \"/content/Scale for Success Part 2_final\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "!zip -r /content/CourseFile.zip \"/content/\"{course}\"_final\"\n",
        "\n",
        "\"\"\"\n",
        "!zip -r /content/CourseFile.zip \"/content/Scale for Success Part 2_final\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVlbUAmjWDnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60fe4135-51fe-498b-83b6-baa77608b808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: /content/Presentation Skills Course_final\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/CourseVideos.zip . -i /content/Presentation Skills Course_final)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/CourseVideos.zip \"/content/Presentation Skills Course_final\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJUPTrFVhJgm"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Book_To_Video_Course_Converter.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDPlEtEHQfTQCir5skhadJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}